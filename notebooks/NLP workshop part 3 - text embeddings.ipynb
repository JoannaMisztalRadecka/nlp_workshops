{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Przetwarzanie-tekstu----text-embeddings\" data-toc-modified-id=\"Przetwarzanie-tekstu----text-embeddings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Przetwarzanie tekstu -  text embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Słowniczek\" data-toc-modified-id=\"Słowniczek-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Słowniczek</a></div><div class=\"lev3 toc-item\"><a href=\"#Importy\" data-toc-modified-id=\"Importy-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Importy</a></div><div class=\"lev2 toc-item\"><a href=\"#Semantyka-dystrybucyjna\" data-toc-modified-id=\"Semantyka-dystrybucyjna-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Semantyka dystrybucyjna</a></div><div class=\"lev2 toc-item\"><a href=\"#Neural-word-embeddings\" data-toc-modified-id=\"Neural-word-embeddings-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Neural word embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Synonimy\" data-toc-modified-id=\"Synonimy-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Synonimy</a></div><div class=\"lev3 toc-item\"><a href=\"#Fasttext\" data-toc-modified-id=\"Fasttext-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Fasttext</a></div><div class=\"lev2 toc-item\"><a href=\"#Analogie\" data-toc-modified-id=\"Analogie-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Analogie</a></div><div class=\"lev3 toc-item\"><a href=\"#Zadanie\" data-toc-modified-id=\"Zadanie-151\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Zadanie</a></div><div class=\"lev3 toc-item\"><a href=\"#Wizualizacja-słów-2D\" data-toc-modified-id=\"Wizualizacja-słów-2D-152\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Wizualizacja słów 2D</a></div><div class=\"lev4 toc-item\"><a href=\"#Słownik-z-odfiltrowanymi-tokenami\" data-toc-modified-id=\"Słownik-z-odfiltrowanymi-tokenami-1521\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>Słownik z odfiltrowanymi tokenami</a></div><div class=\"lev4 toc-item\"><a href=\"#Mapowanie-wektorów-na-2D\" data-toc-modified-id=\"Mapowanie-wektorów-na-2D-1522\"><span class=\"toc-item-num\">1.5.2.2&nbsp;&nbsp;</span>Mapowanie wektorów na 2D</a></div><div class=\"lev4 toc-item\"><a href=\"#Tworzenie-wykresu\" data-toc-modified-id=\"Tworzenie-wykresu-1523\"><span class=\"toc-item-num\">1.5.2.3&nbsp;&nbsp;</span>Tworzenie wykresu</a></div><div class=\"lev2 toc-item\"><a href=\"#Trenowanie-własnego-modelu\" data-toc-modified-id=\"Trenowanie-własnego-modelu-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Trenowanie własnego modelu</a></div><div class=\"lev2 toc-item\"><a href=\"#Reprezentacja-tekstów\" data-toc-modified-id=\"Reprezentacja-tekstów-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Reprezentacja tekstów</a></div><div class=\"lev3 toc-item\"><a href=\"#Reprezentacja-tekstu-jako-średni-wektor-słów\" data-toc-modified-id=\"Reprezentacja-tekstu-jako-średni-wektor-słów-171\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Reprezentacja tekstu jako średni wektor słów</a></div><div class=\"lev3 toc-item\"><a href=\"#Inne-modele-reprezentacji-tekstu\" data-toc-modified-id=\"Inne-modele-reprezentacji-tekstu-172\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Inne modele reprezentacji tekstu</a></div><div class=\"lev3 toc-item\"><a href=\"#Przykład:-szukanie-podobnych-tekstów\" data-toc-modified-id=\"Przykład:-szukanie-podobnych-tekstów-173\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>Przykład: szukanie podobnych tekstów</a></div><div class=\"lev3 toc-item\"><a href=\"#Zadanie\" data-toc-modified-id=\"Zadanie-174\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>Zadanie</a></div><div class=\"lev3 toc-item\"><a href=\"#Rozwiązanie\" data-toc-modified-id=\"Rozwiązanie-175\"><span class=\"toc-item-num\">1.7.5&nbsp;&nbsp;</span>Rozwiązanie</a></div><div class=\"lev2 toc-item\"><a href=\"#Wygląda-fajnie,-ale-po-co-mi-to?\" data-toc-modified-id=\"Wygląda-fajnie,-ale-po-co-mi-to?-18\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Wygląda fajnie, ale po co mi to?</a></div><div class=\"lev3 toc-item\"><a href=\"#Dodatkowe-materiały\" data-toc-modified-id=\"Dodatkowe-materiały-181\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Dodatkowe materiały</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cAP6cdsSwXg"
   },
   "source": [
    "\n",
    "# Przetwarzanie tekstu -  text embeddings\n",
    "\n",
    "W tej części warsztatów dowiesz się:\n",
    "* Czym są \"text embeddings\" i jak zakodować znaczenie słowa\n",
    "* Jak znaleźć synonimy i analogie między słowami z korpusu\n",
    "* Jak znaleźć teksty o podobnym znaczeniu\n",
    "* Jak stworzyć \"semantyczną mapę\" treści"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mnfIcXLSwXh"
   },
   "source": [
    "## Słowniczek\n",
    "* word embeddings - techniki NLP, których rezultatem jest przedstawienie słów w przestrzeni wektorowej (np. reprezentacja BOW)\n",
    "* neural embeddings - modele oparte na sieciach neuronowych, które służą do stworzenia word embeddings (np. model word2vec, fasttext)\n",
    "* redukcja wymiarów - proces zmniejszania liczby wymiarów - przedstawienie danych w innej przestrzeni (służy np. do wybierania istotnych cech lub wizualizacji danych w 2D) Przykładowe algorytmy: PCA (analiza głównych składowych), TSNE (oparte na odwzorowania sąsiedztwa punktów)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FavbOU4-rWHx"
   },
   "source": [
    "### Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T13:40:05.885995Z",
     "start_time": "2018-11-01T13:40:02.930699Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jGkPaJQmSwXs"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import plotly.plotly as py\n",
    "from  plotly import graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKMX6e4ISwX1"
   },
   "source": [
    "## Semantyka dystrybucyjna\n",
    "\n",
    "Założenie: znaczenie słowa jest zdefiniowane przez kontekst, w jakim występuje (sąsiednie wyrazy).\n",
    "\n",
    "Cel: zamiana słów na wektory w taki sposób, żeby wyrazy o podobnym znaczeniu były blisko siebie w przestrzeni wektorowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1C4sqVySwX3"
   },
   "source": [
    "## Neural word embeddings\n",
    "Reprezentacja słów oparta na modelu sieci neuronowej, która przewiduje jakie słowa mogą wystąpić w swoim sąsiedztwie. Uzyskane wagi sieci są używane jako wektorowy słów.\n",
    "\n",
    "Istnieją różne architektury sieci do budowania takich wektorów. Jednym z najpopularniejszych jest Word2Vec (T. Mikolov 2013) - oryginalna publikacja https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "\n",
    "Wyjaśnienie działania: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ \n",
    "\n",
    "Inne popularne modele, które dają lepsze rezultaty dla niektórych zadań to np. GloVe (global vectors) i FastText (uczący się również reprezentacji części słów).\n",
    "\n",
    "Dla wielu modeli i języków (nawet dla polskiego!) są dostępne gotowe, wytrenowane modele i zapisane wektory słów, dla różnych korpusów i rozmiarów wektora. \n",
    "\n",
    "Użycie gotowego modelu znacznie przyspiesza pracę i jest przydatne zwłaszcza, gdy nie mamy wystarczająco dużo danych lub są słabej jakości (np. komentarze użytkowników, nazwy produktów itd.).\n",
    "\n",
    "Skorzystamy z modeli udostępnionych na:\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T13:41:04.631869Z",
     "start_time": "2018-11-01T13:40:20.740480Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oAXkXQ8OSwX4"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100 = api.load(\"glove-wiki-gigaword-100\")  \n",
    "# fasttext_wiki_news_300 = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehFhwET1SwX-"
   },
   "source": [
    "## Synonimy\n",
    "Pierwszą interesującą własnością wektorów słów jest możliwość znajdowania synonimów (jako słów z najbliższą reprezentacją wektorową mierząc metryką cosinusową).\n",
    "\n",
    "Dla lepszej wizualizacji (i ku przestrodze;): https://www.kdnuggets.com/2017/04/cartoon-word2vec-espresso-cappuccino.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T13:41:04.938899Z",
     "start_time": "2018-11-01T13:41:04.633869Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cQ-5nij0SwYF"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T13:41:04.970902Z",
     "start_time": "2018-11-01T13:41:04.940899Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xo3lQC4_SwYM"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100.most_similar(\"spain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T13:41:05.001906Z",
     "start_time": "2018-11-01T13:41:04.973903Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4WpoyYHeSwYU"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100.most_similar(\"vienna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89_xXULCSwYc"
   },
   "source": [
    "### Fasttext\n",
    "\n",
    "Model Fasttext jest trenowany nie tylko dla całych słów, ale też ich fragmentach. \n",
    "\n",
    "Dzięki temu działa dobrze również na tekstach gorszej jakości, z literówkami (np. z Twittera) - umożliwia znalezienie reprezentacji nawet dla słów spoza słownika.\n",
    "\n",
    "Dobrze sprawdza się  też dla języków fleksyjnych (np. dla j. polskiego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:09:13.319720Z",
     "start_time": "2018-11-01T14:09:13.276716Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iHWszDEzSwYd"
   },
   "outputs": [],
   "source": [
    "fasttext_wiki_news_300.most_similar(\"tomorrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w32hWACeSwYm"
   },
   "source": [
    "## Analogie\n",
    "Inną ciekawą własnością takich modeli jest możliwość znajdowania analogii za pomocą prostej arytmetyki na wektorach słów. \n",
    "\n",
    "Dokładniejsze wyjaśnienie dlaczego to działa:\n",
    "\n",
    "http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html\n",
    "\n",
    "Najbardziej znany przykład - znalezienie analogii dla płci, czyli *\"mężczyzna ma się do króla tak jak kobieta do...\"*: \n",
    "\n",
    "**MAN ~ KING**\n",
    "\n",
    "**WOMAN ~ ??**\n",
    "\n",
    "Czyli:\n",
    "\n",
    "**KING - MAN + WOMAN = ?**\n",
    "\n",
    "Korzystamy z funkcji *most_similar*, podając słowa \"dodatnie\" i \"ujemne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:09:20.480436Z",
     "start_time": "2018-11-01T14:09:20.421431Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aS-i-IyBSwYq"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100.most_similar([\"king\", \"woman\"], [\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWkIkzSLSwY1"
   },
   "source": [
    "BERLIN - GERMANY + POLAND = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:09:21.524541Z",
     "start_time": "2018-11-01T14:09:21.477536Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ndAAchY8SwY3"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100.most_similar([\"berlin\", \"poland\"], [\"germany\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r67qRY_ZSwZB"
   },
   "source": [
    "TRUMP - USA + RUSSIA = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:09:22.993688Z",
     "start_time": "2018-11-01T14:09:22.934682Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CQQWD6jTSwZD"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100.most_similar([\"trump\", \"russia\"], [\"usa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PNkWIxwSwZP"
   },
   "source": [
    "SPAIN - EUROPE + ASIA = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:09:23.553744Z",
     "start_time": "2018-11-01T14:09:23.502739Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8WM0WRJaSwZS"
   },
   "outputs": [],
   "source": [
    "glove_wiki_100.most_similar([\"spain\", \"asia\"], [\"europe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hChBVXzpSwZZ"
   },
   "source": [
    "DOG - ADULT + CHILD = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:09:39.735362Z",
     "start_time": "2018-11-01T14:09:39.676356Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "acJ_XXFDSwZb"
   },
   "outputs": [],
   "source": [
    "fasttext_wiki_news_300.most_similar([\"dog\", \"child\"], [\"adult\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZE4uEg6SwaG"
   },
   "source": [
    "### Zadanie\n",
    "Korzystając z modelu glove_wiki_100 znajdź:\n",
    "\n",
    "PAST ~ YESTERDAY\n",
    "\n",
    "FUTURE ~ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_uL21wj7SwaH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAcUXqEYSwaL"
   },
   "source": [
    "### Wizualizacja słów 2D\n",
    "\n",
    "Wektorową reprezentację słów możemy wykorzystać również do wizualizacji semantycznych relacji między słowami. W tym celu potrzebujemy przedstawić wektory w przestrzeni 2D, którą można pokazać na wykresie - użyjemy w tym celu algorytmów redukcji wymiarów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZwSgXZlGSwaM"
   },
   "source": [
    "#### Słownik z odfiltrowanymi tokenami\n",
    "\n",
    "Chcemy stworzyć wizualizację dla słów z naszego korpusu (baza artykułów).\n",
    "\n",
    "Wizualizujemy tylko 1000 popularnych słów po oczyszczeniu i odfiltrowaniu stopwords.\n",
    "\n",
    "Używamy reprezentacji GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:09:57.411129Z",
     "start_time": "2018-11-01T14:09:57.393127Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "V3ZbW2WISwaN"
   },
   "outputs": [],
   "source": [
    "def preprocess_tokens(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.tokenize.regexp_tokenize(text, '[a-zA-Z]{3,}')\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:10:15.125900Z",
     "start_time": "2018-11-01T14:10:02.534641Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KMgYVEYfSwaQ"
   },
   "outputs": [],
   "source": [
    "articles = pd.read_csv('articles1.csv.zip', compression='zip')\n",
    "articles_sample = articles.sample(n=2000)\n",
    "\n",
    "tokens = articles_sample.content.apply(preprocess_tokens)\n",
    "dct = Dictionary(tokens)\n",
    "dct.filter_extremes(no_below=10, no_above=0.5)\n",
    "dct.filter_extremes(keep_n=1000)\n",
    "word_vecs = [glove_wiki_100[w] for w in dct.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rhxyf5ToSwaU"
   },
   "source": [
    "#### Mapowanie wektorów na 2D\n",
    "\n",
    "Do wizualizacji użyjemy mapowania wektorów na przestrzeń 2D za pomocą algorytmu TSNE.\n",
    "\n",
    "Jest to metoda nieliniowej redukcji wymiarów, w wyniku której wektory, które były blisko w początkowej wielowymiarowej przestrzeni będą blisko również po transformacji. Więcej o algorytmie w dokumentacji sklearn: http://scikit-learn.org/stable/modules/manifold.html#t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:10:50.072395Z",
     "start_time": "2018-11-01T14:10:19.344322Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ak6hZRpvSwaV"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "words_2d = tsne.fit_transform(word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qS6DVWsUSwaY"
   },
   "source": [
    "#### Tworzenie wykresu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:10:50.190407Z",
     "start_time": "2018-11-01T14:10:50.076395Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oXIcK_1KSwaZ"
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(            \n",
    "            x = words_2d[:,0],            \n",
    "            y = words_2d[:,1],           \n",
    "            mode = 'markers', \n",
    "            text =  list(dct.values()), \n",
    "            name = \"word\")\n",
    "iplot([trace], filename='glove_vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCIrl5NKSwah"
   },
   "source": [
    "## Trenowanie własnego modelu\n",
    "\n",
    "Jeśli mamy odpowiednio dużo danych, możemy wytrenować model reprezentacji słów na własnym korpusie. Takie wektory powinny lepiej reprezentować charakterystykę danego korpusu (szczególnie dla specjalistycznych tekstów z danej dziedziny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:15:16.265011Z",
     "start_time": "2018-11-01T14:10:50.193407Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K1Qr4cg6Swaj"
   },
   "outputs": [],
   "source": [
    "articles['tokens'] = articles.content.apply(preprocess_tokens)\n",
    "w2v = Word2Vec(articles['tokens'], size=100, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:15:46.201005Z",
     "start_time": "2018-11-01T14:15:16.268012Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "C3FJVmz4Swal"
   },
   "outputs": [],
   "source": [
    "word_vecs_custom = [w2v[w] for w in dct.values()]\n",
    "tsne = TSNE()\n",
    "words_2d_custom = tsne.fit_transform(word_vecs_custom)\n",
    "trace = go.Scatter(            \n",
    "            x = words_2d_custom[:,0],            \n",
    "            y = words_2d_custom[:,1],           \n",
    "            mode = 'markers', \n",
    "            text =  list(dct.values()), \n",
    "            name = \"word\")\n",
    "\n",
    "iplot([trace], filename='words-scatter-custom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFVuYV4ESwaq"
   },
   "source": [
    "## Reprezentacja tekstów\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgsXMF49Swar"
   },
   "source": [
    "### Reprezentacja tekstu jako średni wektor słów\n",
    "\n",
    "Wektory słów mogą nam posłużyć również do reprezentacji całych tekstów. W tym celu możemy obliczyć średni wektor z reprezentacji słów wstępujących w dokumencie (np. po odfiltrowaniu stopwordsów). \n",
    "Takie rozwiązanie będzie dobre szczególnie jeśli nasz zbiór dokumentów jest zbyt mały lub zbyt niskiej jakości, żeby użyć go trenowania osobnego modelu.\n",
    "\n",
    "Tego typu podejście jest też często używane jako dobry baseline dla bardziej zaawansowanych metod reprezentacji dokumentów. Więcej o najnowszych metodach reprezentacji tektów i słów: https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:17:01.568541Z",
     "start_time": "2018-11-01T14:15:46.203005Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8RlGkVPSSwas"
   },
   "outputs": [],
   "source": [
    "# Ze względu na wydajność i przjrzystość wizualizacji, pokażemy tylko próbkę tekstów\n",
    "articles_sample = articles.sample(n=2000)\n",
    "articles_w2v = articles_sample['tokens'].apply(lambda x: np.mean([w2v[w] for w in x if w in w2v], 0))\\\n",
    "                                        .apply(pd.Series).set_index(articles_sample.index).dropna()\n",
    "\n",
    "tsne = TSNE()\n",
    "articles_2d = pd.DataFrame(tsne.fit_transform(articles_w2v), index=articles_w2v.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HIK7y-4Swaw"
   },
   "source": [
    "Na wykresie osobnymi kolorami zaznaczymy artykuły z różnych źródeł."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T14:17:01.786562Z",
     "start_time": "2018-11-01T14:17:01.571541Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zmc_q1RhSwaw"
   },
   "outputs": [],
   "source": [
    "traces = []\n",
    "publishers = articles.publication.unique()\n",
    "for publisher in publishers:\n",
    "    articles_pub = articles_sample[articles_sample.publication==publisher]\n",
    "    traces.append(go.Scatter(            \n",
    "                x = articles_2d.loc[articles_pub.index][0],            \n",
    "                y = articles_2d.loc[articles_pub.index][1],           \n",
    "                mode = 'markers', \n",
    "                text =  articles_pub.title, \n",
    "                name =publisher))\n",
    "    \n",
    "iplot(traces, filename='articles-w2v-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RvII3ybSway"
   },
   "source": [
    "### Inne modele reprezentacji tekstu\n",
    "\n",
    "Jeśli mamy duży zbiór dobrej jakości tekstów, możemy wytrenować model Doc2Vec, który uczy się jednocześnie reprezentacji słów i całych tekstów. \n",
    "\n",
    "Taka reprezentacja powinna dawać lepsze rezultaty niż średni wektor np. dla znajdowania podobnych tekstów i klasyfikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTkQKlRJSwaz"
   },
   "outputs": [],
   "source": [
    "articles_tagged_doc = articles.apply(lambda x: TaggedDocument(x.tokens, [x.id]), 1)\n",
    "d2v = Doc2Vec(articles_tagged_doc, size=100, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_zVtrx8Swa1"
   },
   "source": [
    "### Przykład: szukanie podobnych tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUbxGtocSwa2"
   },
   "outputs": [],
   "source": [
    "examples = articles.sample(n=5)\n",
    "for article in examples.iterrows():\n",
    "    print('\\n\\nSimilar to: {}\\n'.format(article[1].title))\n",
    "    for similar in d2v.docvecs.most_similar(article[1].id):\n",
    "        print(articles[articles.id==similar[0]].title.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzt5EaSISwa4"
   },
   "source": [
    "### Zadanie\n",
    "\n",
    "Ściągnij z Kaggle opisy projektów \"kickstarter\": https://www.kaggle.com/kemical/kickstarter-projects\n",
    "Użyj gotowych modeli word embeddings (np. GloVe) i stwórz wizualizację 2D projektów na podstawie ich opisów (kolumna \"name\").\n",
    "\n",
    "Użyj różnych serii(kolorów) do reprezentacji pozczególnych kategorii projektów (kolumna \"category\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtDejFouR9fA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6prD8rhSwa4"
   },
   "source": [
    "## Wygląda fajnie, ale po co mi to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAoSK_d7Swa5"
   },
   "source": [
    "Wektory słów i tekstów mogą być przydatne nie tylko do wizualizacji. Oprócz wyszukiwania synonimów i podobnych tekstów można użyć ich także jako reprezentacji wejściowej dla innych modeli, np. do klastrowania lub klasyfikacji tekstów. \n",
    "\n",
    "Wtedy wektory można wytrenować na niezależnym, większym korpusie o dobrej jakości, a następnie uczyć model na mniejszej próbce danych z daną kategorią. Jest to przykład transfer learningu.\n",
    "\n",
    "Modele typu \"all2vec\" znalazły również zastosowanie w wielu innych dziedzinach. Ciekawym przykładem są systemy rekomendacyjne, w których użytkownicy lub przedmioty są reprezentowane jako słowa w korpusie, a ich sekwencje jako zdania. \n",
    "Przykłady wykorzystania takiego podejścia: http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQJ_eW-SqxXT"
   },
   "source": [
    "### Dodatkowe materiały\n",
    "\n",
    "* Pretrenowane wektory (word2vec) dla j. polskiego: http://dsmodels.nlp.ipipan.waw.pl/w2v.html\n",
    "* Wektory fasttext wytrenowane dla 157 języków (w tym j. polskiego): https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWwbd1a_q2nF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP workshops part 3 - text embeddings.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "425px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
